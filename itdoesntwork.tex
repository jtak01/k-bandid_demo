\documentclass{article}
\usepackage[utf8]{inputenc}

\title{COMP 138 RL: Homework Template}
\author{Firstname Lastname}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Goals}
The goal of this assignment is to explore the inadequacies of the sample average method in non-stationary problems. Additionally, multiple feedback techniques will be used (ie, e-greedy, greedy, optimistic initial value, and Upper bound selection) on both stationary and non-stationary settings in order to illuminate the differences between the feedback techniques.


\section{Introduction}
We will define the stationary K-bandit problem as follows: There are k different actions to take. These actions will return some reward value which is chosen from a constant, normal Gaussian distribution with mean = 0 and standard deviation = 1. Action is chosen based on metrics derived from several approaches (sample mean, recency bias). Furthermore with multiple feedback techniques we will see a wide range of balance between exploration and exploitation which hopefully will provide insights into approaches in (non) stationary environments. 



\section{Different Action Selections}
e-greedy: 
Agent balances exploration and exploitation in this method. There is  chance that the agent will try a completely random action. With 1- chance the agent will pick the optimal action (defined by optimal action being the highest q* value) based on expected reward. 




\bibliographystyle{plain}
\bibliography{references}
\end{document}
